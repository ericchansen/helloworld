{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Next Purchase\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial will walk you through using a historical sales dataset to predict whether or not a customer will **spend more than $500 in the next month**. This is a supervised classification problem.\n",
    "\n",
    "Data is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/). This is a transnational [data set](https://archive.ics.uci.edu/ml/datasets/online+retail) which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "<font color='red'>WARNING!</font> For this demo to work, you can't use any commits of `featuretools` beyond `ffb8081ca98be59b8e053ebc4e34adeecc9f32ab`. To install a particular version of a Python package from GitHub, you can use the following command. I'm not aware of a way to do this using the Anaconda Navigator GUI.\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/Featuretools/featuretools@ffb8081ca98be59b8e053ebc4e34adeecc9f32ab\n",
    "```\n",
    "\n",
    "## Contents\n",
    "\n",
    "---\n",
    "\n",
    "- [Setup](#Setup)\n",
    "  - [Imports](#Imports)\n",
    "  - [Visual Setup](#Visual-Setup)\n",
    "  - [Functions](#Functions)\n",
    "  - [Load Data](#Load-Data)\n",
    "  - [Data Cleaning and Exploration](#Data-Cleaning-and-Exploration)\n",
    "  - [Empirical Cumulative Distribution Function](#Empirical-Cumulative-Distribution-Function)\n",
    "- [Prediction Problem](#Prediction-Problem)\n",
    "- [Making Labels](#Making-Labels)\n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "  - [Featuretools Implementation](#Featuretools-Implementation)\n",
    "    - [Create an Entity Set](#Create-an-Entity-Set)\n",
    "    - [Normalize Entities](#Normalize-Entities)\n",
    "    - [Deep Feature Synthesis](#Deep-Feature-Synthesis)\n",
    "- [Preliminary Modelling](#Preliminary-Modelling)\n",
    "- [Compare to Baseline](#Compare-to-Baseline)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "---\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.distributions.empirical_distribution import ECDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Setup\n",
    "\n",
    "Below is what I call Jupyter Notebook magic. This code is specific to Jupyter Notebooks, and code like this is prefixed by the `%` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure looks pretty, doesn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAABQCAYAAADiBIpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAAppJREFUeJzt3a1uFFEcxuH/0Hb7CVgMF0AVDlOFwDdU9ALQXEw1F4CAIEkQCOg1bILGgKTpV7Y0g2kQr2PTk1Mmz2OOmuTdHDG/ZMQO4ziOBQDAX/d6DwAAuGsEEgBAEEgAAEEgAQAEgQQAEAQSAEAQSAAAQSABAASBBAAQBBIAQBBIAABBIAEAhNVlH/w4v6jzxTT/5/bl062qz0e9Z7Tz/HW9+fm+94omXn29qjo4rHr3tveUNg4Oa37ce0Q7u3tV348nendV9XjvsK4+/eg9o5m1F49qPp/3ntHE7pNvVcN+1fih95Q2hv36cnrSe0UT68NQz7bv//NzSwfS+WKss4kGUlVVXfzqvaCpk+uz3hPaOF3cnKd9dzR0ddl7QVu/L6d7d1VVdXHde0FTi8Wi94RGzuKcnstxwu/0JfjEBgAQBBIAQBBIAABBIAEABIEEABAEEgBAEEgAAEEgAQAEgQQAEAQSAEAQSAAAQSABAASBBAAQBBIAQBBIAABBIAEABIEEABAEEgBAEEgAAEEgAQAEgQQAEAQSAEAQSAAAQSABAASBBAAQBBIAQBBIAABBIAEABIEEABAEEgBAEEgAAEEgAQAEgQQAEAQSAEAQSAAAQSABAASBBAAQBBIAQBBIAABBIAEABIEEABAEEgBAEEgAAEEgAQAEgQQAEAQSAEAQSAAAQSABAASBBAAQBBIAQBBIAABhddkHt2bDbe64ezYf9l7Q1IOV7d4T2tiZ3Zw7fXc0tLbRe0FbqxvTvbuqqtpc6b2gqdls1ntCI9txTs/GMM33+vqSv2sYx3G85S0AAP81n9gAAIJAAgAIAgkAIAgkAIAgkAAAgkACAAgCCQAgCCQAgCCQAACCQAIACAIJACAIJACA8AeSD0aU8O45+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(palette=\"pastel\")\n",
    "current_palette = sns.color_palette()\n",
    "sns.palplot(current_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "I'm defining a variety of functions that will be used later. For the reader, you can just skip past this section for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, n=15, threshold=None):\n",
    "    \"\"\"Plots n most important features.\n",
    "    \n",
    "    Also plots the cumulative importance i threshold is specified and prints the number of\n",
    "    features needed to reach threshold cumulative importance. Intended for use with any\n",
    "    tree-based feature importances. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Dataframe of feature importances. Columns must be \"feature\"\n",
    "                        and \"Importance\".\n",
    "    \n",
    "        n (int): Number of most important features to plot. Default is 15.\n",
    "    \n",
    "        threshold (float): Threshold for cumulative importance plot. If not provided,\n",
    "                           no plot is made. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        df (dataframe): Dataframe ordered by feature importances with a normalized\n",
    "                        column (sums to 1) and a cumulative importance column\n",
    "    \n",
    "    Note:\n",
    "    \n",
    "        * Normalization in this case means sums to 1. \n",
    "        * Cumulative importance is calculated by summing features from most to least\n",
    "          important.\n",
    "        * A threshold of 0.9 will show the most important features needed to reach 90% of\n",
    "          cumulative importance.    \n",
    "    \"\"\"\n",
    "    # Sort features with most important at the head.\n",
    "    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Normalize the feature importances to add up to one and calculate cumulative importance.\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    # Bar plot of n most important features.\n",
    "    df.loc[:n, :].plot.barh(\n",
    "        y = 'importance_normalized', \n",
    "        x = 'feature',\n",
    "        color = 'blue', \n",
    "        edgecolor = 'k',\n",
    "        figsize = (12, 8),\n",
    "        legend = False\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n",
    "    plt.title(f'Top {n} Most Important Features', size = 18)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    \n",
    "    if threshold:\n",
    "        # Cumulative importance plot.\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n",
    "        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n",
    "        plt.title('Cumulative Feature Importance', size = 18);\n",
    "        \n",
    "        # Number of features needed for threshold cumulative importance.\n",
    "        # This is the index (will need to add 1 for the actual number).\n",
    "        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "        \n",
    "        # Add vertical line to plot.\n",
    "        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n",
    "        plt.show();\n",
    "        \n",
    "        print('{} features required for {:.0f}% of cumulative importance.'.format(\n",
    "            importance_index + 1,\n",
    "            100 * threshold\n",
    "        ))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote out how to do this manually later in the notebook for the 1st model developed. This code makes iterating and testing other models much more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(\n",
    "    model,\n",
    "    time_period_to_predict,\n",
    "    feature_matrix,\n",
    "    verbose=True\n",
    "):\n",
    "    # For now, this is just taking the int of the month. We could make this smarter.\n",
    "    month = time_period_to_predict\n",
    "\n",
    "    test_labels = feature_matrix.loc[feature_matrix['Month'] == month, 'Label']\n",
    "    train_labels = feature_matrix.loc[feature_matrix['Month'] < month, 'Label']\n",
    "    \n",
    "    # Remember that the training and test set are dependent upon the timeframe.\n",
    "    x_train = feature_matrix[feature_matrix['time'].dt.month < month].drop(columns = [\n",
    "        \"CustomerID\",\n",
    "        \"time\",\n",
    "        \"Month\",\n",
    "        \"Label\",\n",
    "        \"Total\"\n",
    "    ])\n",
    "    x_test = feature_matrix[feature_matrix['time'].dt.month == month].drop(columns = [\n",
    "        \"CustomerID\",\n",
    "        \"time\",\n",
    "        \"Month\",\n",
    "        \"Label\",\n",
    "        \"Total\"\n",
    "    ])\n",
    "    \n",
    "    # Impute and scale features.\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', Imputer(strategy='median')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    # Fit and transform training and test data.\n",
    "    transformed_x_train = pipeline.fit_transform(x_train)\n",
    "    transformed_x_test = pipeline.fit_transform(x_test)\n",
    "    \n",
    "    # Get labels.\n",
    "    y_train = np.array(train_labels).reshape((-1, ))\n",
    "    y_test = np.array(test_labels).reshape((-1, ))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Number of training observations: {}\".format(len(transformed_x_train)))\n",
    "        print(\"Number of training labels: {}\".format(len(y_train)))\n",
    "    model.fit(transformed_x_train, y_train)\n",
    "    return model, transformed_x_train, transformed_x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_informed_baseline(\n",
    "    time_period_to_predict,\n",
    "    labels\n",
    "):\n",
    "    baseline_labels = labels.copy()\n",
    "    baseline_labels['Month'] = baseline_labels['CutoffTime'].dt.month\n",
    "\n",
    "    previous_month_baseline_labels = baseline_labels[baseline_labels['Month'] == (time_period_to_predict - 1)]\n",
    "    prediction_month_baseline_labels = baseline_labels[baseline_labels['Month'] == 6]\n",
    "\n",
    "    previous_month_baseline_labels = previous_month_baseline_labels.rename(columns = {'Total': 'PreviousMonthTotal'})\n",
    "    prediction_month_baseline_labels = prediction_month_baseline_labels.rename(columns = {'Total': 'PredictionMonthTotal'})\n",
    "\n",
    "    prediction_month_baseline_labels = prediction_month_baseline_labels.merge(previous_month_baseline_labels[['CustomerID', 'PreviousMonthTotal']], on='CustomerID', how='left')\n",
    "    prediction_month_baseline_labels['PreviousMonthTotal'] = prediction_month_baseline_labels['PreviousMonthTotal'].fillna(0)\n",
    "    prediction_month_baseline_labels['PredictedLabel'] = (prediction_month_baseline_labels['PreviousMonthTotal'] > 500).astype(int)\n",
    "\n",
    "    prediction_month_baseline_labels['Probability'] = prediction_month_baseline_labels['PreviousMonthTotal'] / 500\n",
    "\n",
    "    # Set probabilities greater than 1 equal to 1.\n",
    "    prediction_month_baseline_labels.loc[prediction_month_baseline_labels['Probability'] > 1, 'Probability'] = 1\n",
    "\n",
    "    prediction_month_baseline_labels.sample(25, random_state=50)\n",
    "    return prediction_month_baseline_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(truth, predictions, verbose=True):\n",
    "    metrics_precision = precision_score(truth, predictions)\n",
    "    metrics_recall = recall_score(truth, predictions)\n",
    "    metrics_f1 = f1_score(truth, predictions)\n",
    "    metrics_roc_auc = roc_auc_score(truth, predictions)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Precision: {}\".format(metrics_precision))\n",
    "        print(\"Recall: {}\".format(metrics_recall))\n",
    "        print(\"F1 score: {}\".format(metrics_f1))\n",
    "        print(\"ROC AUC: {}\".format(metrics_roc_auc))\n",
    "    \n",
    "    return metrics_precision, metrics_recall, metrics_f1, metrics_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_baseline_to_auto(\n",
    "    time_period_to_predict,\n",
    "    feature_matrix,\n",
    "    labels\n",
    "):\n",
    "    baseline_df = get_informed_baseline(time_period_to_predict, labels)\n",
    "    print(\"Baseline:\")\n",
    "    _ = get_performance_metrics(baseline_df['Label'], baseline_df['PredictedLabel'])\n",
    "    \n",
    "    random_forest_model = RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        random_state=50,\n",
    "        n_jobs=-1\n",
    "    )   \n",
    "    random_forest_model, transformed_x_train, transformed_x_test, y_train, y_test = get_trained_model(\n",
    "        model=random_forest_model, \n",
    "        time_period_to_predict=time_period_to_predict,\n",
    "        feature_matrix=feature_matrix,\n",
    "        verbose=False\n",
    "    )\n",
    "    random_forest_model_predictions = random_forest_model.predict(transformed_x_test)\n",
    "    random_forest_model_probabilities = random_forest_model.predict_proba(transformed_x_test)[:, 1]\n",
    "    print(\"\\nPipeline:\")\n",
    "    _ = get_performance_metrics(truth=y_test, predictions=random_forest_model_predictions)\n",
    "    \n",
    "    base_fpr, base_tpr, _ = roc_curve(baseline_df['Label'], baseline_df['Probability'])\n",
    "    model_fpr, model_tpr, _ = roc_curve(feature_matrix[feature_matrix['Month'] == time_period_to_predict]['Label'], random_forest_model_probabilities)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "`pd` stands for the [Pandas](https://pandas.pydata.org/) library. Pandas it the de facto way to deal with tables in Python. Remember how we did `import pandas as pd` above? That's just to save space. Now we can type `pd` instead of `pandas`. Wow, Python is convenient! Nice!\n",
    "\n",
    "Note that Pandas gets thrown out the window when we talk about really, really big. That being said, the packages that deal with big data, such as PySpark, do their best to replicate the Pandas syntax. What works with a Pandas dataframe (that's our lingo for table) often works with a PySpark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/Online Retail.xlsx\", parse_dates=[\"InvoiceDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Exploring\n",
    "\n",
    "So how much data do we really have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some customer ID's are missing. That's no good. Get rid of those rows.\n",
    "\n",
    "Here, `axis` refers to the axis of the table. There's two dimensions here, so `axis=0` means drop rows. `axis=1` would drop columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check for duplicates. Yep, we had some. Got to love dirty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to restrict our scope for prototyping. Let's stick to 2011 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"InvoiceDate\"].dt.year == 2011]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price is in sterlings. Let's convert that to dollars. We're just going estimate here that 1 pound sterling can be exchanged for 1.31 US dollars. A more accurate approach would be to use the conversion rate on the day of the invoice.\n",
    "\n",
    "Since we're using the same column for the output as the input, we're rewritting the values in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UnitPrice'] = df['UnitPrice'] * 1.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column that contains the total.\n",
    "\n",
    "Here, you can see that we're adding a new column because `df['Total']` doesn't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total'] = df['UnitPrice'] * df['Quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did I mention that you should always read the documentation? That includes reading about your data! From the [provided descriptions](https://archive.ics.uci.edu/ml/datasets/online+retail),\n",
    "\n",
    "> InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n",
    "\n",
    "That's a useful piece of information! Good thing we did our due research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cancelled'] = df[\"InvoiceNo\"].str.startswith(\"C\", na=False)\n",
    "df['Cancelled'].value_counts().plot.bar()\n",
    "plt.title(\"Cancelled Purchases Breakdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few of these. All the cancelled orders have negative quantities. Now that wasn't explained in the descriptions! Let's leave in the cancelled orders because if our goal is to predict the total amount purchased by a customer, we'll need to take into account their cancelled orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Cancelled'] == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe` function is quite useful. It acts upon all of our numeric columns (notice that data types were automatically inferred when we read in the Excel). Most totals are around $27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slice by countries. We can see that the purchase total and quantities are heavily skewed. This might make predicting the actual spending amount difficult for a regression approach. One alternative would be to frame the problem as a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x='Country', y='Total', data=df[(df['Total'] > 0) & (df['Total'] < 1000)])\n",
    "plt.title(\"Total Purchase Amount by Country\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x='Country', y='Quantity', data=df[(df['Total'] > 0) & (df['Total'] < 1000)])\n",
    "plt.title(\"Quantity Purchased by Country\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Cumulative Distribution Function\n",
    "\n",
    "An empirical cumulative distribution function (ECDF) plot can help us visualize the extent of how skewed our data is. The ECDF is defined as\n",
    "\n",
    "<center>$$\\hat{F}_{n}(t) = \\frac{\\text{number of elements in the sample } \\leq \\text{ }t}{n} = \\frac{1}{n}\\sum_{i=1}^{n} 1_{x_{1} \\leq t}$$</center>\n",
    "\n",
    "where $1_{A}$ is the indicator of event $A$.\n",
    "\n",
    "Here's an example of an ECDF overlaid with a random sampling from a normal distribution with $\\mu = 0$ and $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_from_normal_dist = np.random.normal(size=10000)\n",
    "ecdf_of_normal = ECDF(samples_from_normal_dist)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(121)\n",
    "sns.distplot(samples_from_normal_dist, label=\"Sampling from Normal Distribution\")\n",
    "sns.lineplot(ecdf_of_normal.x, ecdf_of_normal.y, label=\"ECDF\")\n",
    "plt.title(\"Normal Distribution and ECDF\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the ECDF to our order data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecdf_of_total = ECDF(df.loc[df['Total'] > 0, 'Total'])\n",
    "ecdf_of_quantity = ECDF(df.loc[df['Total'] > 0, 'Quantity'])\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(121)\n",
    "plt.plot(ecdf_of_total.x, ecdf_of_total.y, marker='o')\n",
    "plt.title(\"ECDF of Purchase Total\")\n",
    "plt.xlabel(\"Total (US Dollars)\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(ecdf_of_quantity.x, ecdf_of_total.y, marker='o')\n",
    "plt.title(\"ECDF of Purchase Quantity\")\n",
    "plt.xlabel(\"Quantity\")\n",
    "plt.ylabel(\"Percentile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty skewed to me. Rather than using regression, let's frame this as a classification problem.\n",
    "\n",
    "## Prediction Problem\n",
    "\n",
    "---\n",
    "\n",
    "Ultimately, the goal of supervised machine learning is to predict some quantity (regression) or a label (classification). Our data exploration showed that regression isn't the best approach due to the extreme outliers, so instead let's make a classification problem.\n",
    "\n",
    "With this dataset, we could frame an unlimited number of classification problems because there are no labels. Choosing a worthwhile quantity to predict therefore becomes critical. In most real-world situations, we could use a domain expert to frame a problem based on what they know is important in the field, and then it's our objective to make a set of labels and features based on that problem. This is known as prediction engineering. We'll frame the problem as predicting whether or not a customer will **spend more than $500 in the next month**.\n",
    "\n",
    "Instead of picking just a single month for predictions, we can use each customer as a label multiple times. In other words, we not only predict whether a given customer will spend more than $500 in May, but we also ask the same question in June, July, and so on. The thing to note is that for each month, we can't use data from the future to predict the class of spending. Each month we can use information from any previous month which means that our predictions should get more accurate as we advance further in time through the data since we'll be able to use more information. Each label for a customer therefore has a different set of features because there is more or less data available to us depending on the month. Doing this by hand is very tedious and error-prone, so let's use `featuretools` to help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Labels\n",
    "\n",
    "---\n",
    "\n",
    "The function below takes a start and a end date and generates a dataframe of the labels, which depends on how much the customer spent in the period and the thershold.\n",
    "\n",
    "For customers who appear in the data prior to the start date but then do have a purchase in between the start and end date, we set their total to 0. If we simply did not include them in the labels, then that would be cheating since we have no way of knowing ahead of time that they will not spend anything in the next month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labelled_data(start_date, end_date, threshold=500):\n",
    "    customers_before_start_date = df[df['InvoiceDate'] < start_date]['CustomerID'].unique()\n",
    "    total_spent_in_time_frame = df[\n",
    "        df['CustomerID'].isin(customers_before_start_date) &\n",
    "        (df['InvoiceDate'] > start_date) &\n",
    "        (df['InvoiceDate'] < end_date)\n",
    "    ].groupby('CustomerID')['Total'].sum().reset_index()\n",
    "    table_with_only_customer_id = pd.DataFrame({'CustomerID': customers_before_start_date})\n",
    "    total_spent_in_time_frame = total_spent_in_time_frame.merge(table_with_only_customer_id, on='CustomerID', how='right')\n",
    "    total_spent_in_time_frame[\"CutoffTime\"] = pd.to_datetime(start_date)\n",
    "    total_spent_in_time_frame[\"Total\"] = total_spent_in_time_frame[\"Total\"].fillna(0)\n",
    "    total_spent_in_time_frame[\"Label\"] = (total_spent_in_time_frame[\"Total\"] > threshold).astype(int)\n",
    "    total_spent_in_time_frame = total_spent_in_time_frame[[\"CustomerID\", \"CutoffTime\", \"Total\", \"Label\"]]\n",
    "    return total_spent_in_time_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data_for_may = make_labelled_data(pd.datetime(2011, 5, 1), pd.datetime(2011, 6, 1))\n",
    "labelled_data_for_may.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each customer who appeared in the data before May, we have a label for them for the month of May which is the sum of their spending in May converted to a binary label. When we make features for these labels, we can only use data from before May. `CutoffTime` represents the point at which any data we use must come before and the label is based on our threshold of $500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data_for_may['Label'].value_counts().plot.bar()\n",
    "plt.title(\"Label Distribution for May\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.xticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data_for_march = make_labelled_data(pd.datetime(2011, 3, 1), pd.datetime(2011, 4, 1))\n",
    "labelled_data_for_april = make_labelled_data(pd.datetime(2011, 4, 1), pd.datetime(2011, 5, 1))\n",
    "# May was done above.\n",
    "labelled_data_for_june = make_labelled_data(pd.datetime(2011, 6, 1), pd.datetime(2011, 7, 1))\n",
    "labelled_data_for_july = make_labelled_data(pd.datetime(2011, 7, 1), pd.datetime(2011, 8, 1))\n",
    "labelled_data_for_august = make_labelled_data(pd.datetime(2011, 8, 1), pd.datetime(2011, 9, 1))\n",
    "labelled_data_for_september = make_labelled_data(pd.datetime(2011, 9, 1), pd.datetime(2011, 10, 1))\n",
    "labelled_data_for_october = make_labelled_data(pd.datetime(2011, 10, 1), pd.datetime(2011, 11, 1))\n",
    "labelled_data_for_november = make_labelled_data(pd.datetime(2011, 11, 1), pd.datetime(2011, 12, 1))\n",
    "labelled_data_for_december = make_labelled_data(pd.datetime(2011, 12, 1), pd.datetime(2012, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 28,000 labels with about 17% being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat(\n",
    "    [\n",
    "        labelled_data_for_march,\n",
    "        labelled_data_for_april,\n",
    "        labelled_data_for_may,\n",
    "        labelled_data_for_june,\n",
    "        labelled_data_for_july,\n",
    "        labelled_data_for_august,\n",
    "        labelled_data_for_september,\n",
    "        labelled_data_for_october,\n",
    "        labelled_data_for_november,\n",
    "        labelled_data_for_december\n",
    "    ],\n",
    "    axis=0)\n",
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels = labels.copy()\n",
    "plot_labels[\"Month\"] = plot_labels[\"CutoffTime\"].dt.month\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    x=\"Month\",\n",
    "    y=\"Total\",\n",
    "    data=plot_labels[(plot_labels[\"Total\"] > 0) & (plot_labels[\"Total\"] < 1000)])\n",
    "plt.title(\"Customer Spending Distribution by Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dive into one customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = 12347\n",
    "labels.loc[labels['CustomerID'] == customer_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.loc[labels['CustomerID'] == customer_id].set_index('CutoffTime')['Total'].plot(figsize=(6, 4))\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Spending (US Dollars)\")\n",
    "plt.title(\"Monthly Spending for Customer {}\".format(customer_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "Machine learning? There's a library for that (and there's a lot of them). Feature engineering is the hard part of the battle. Fortunately, there's even a library for that now!\n",
    "\n",
    "### Featuretools Implementation\n",
    "\n",
    "#### Create an Entity Set\n",
    "\n",
    "Think of the `EntitySet` as a collection of tables that are all related to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet(id=\"Online Retail Logs\")\n",
    "es.entity_from_dataframe(\n",
    "    \"Purchases\",\n",
    "    dataframe=df,\n",
    "    index=\"PurchasesIndex\",\n",
    "    time_index=\"InvoiceDate\",\n",
    "    variable_types={\n",
    "        \"InvoiceNo\": ft.variable_types.Categorical,\n",
    "        \"StockCode\": ft.variable_types.Categorical,\n",
    "        \"Description\": ft.variable_types.Text,\n",
    "        \"Quantity\": ft.variable_types.Numeric,\n",
    "        # \"InvoiceDate\": ft.variable_types.Datetime,\n",
    "        \"UnitPrice\": ft.variable_types.Numeric,\n",
    "        \"CustomerID\": ft.variable_types.Numeric,\n",
    "        \"Country\": ft.variable_types.Categorical,\n",
    "        \"Total\": ft.variable_types.Numeric,\n",
    "        \"Cancelled\": ft.variable_types.Boolean\n",
    "    }\n",
    ")\n",
    "es[\"Purchases\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create tables out of the original table/entity by normalizing. This creates new tables by creating a unique row for every variable we pass in, such as the customer or product.\n",
    "\n",
    "The code below creates a new entity for the products where the grain is one product per row and the columns describe the product. `normalize_entity` automatically creates the relationships and time index. If we want to include any additional variables in the table, we can pass those in. Here we add the description.\n",
    "\n",
    "Notice that the first purchase time is automatically created because the purchases table has a time index. This represents the first time the product appears in the purchase data. Featuretools automatically filters data from this table for each label so taht we only build valid features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.normalize_entity(\n",
    "    new_entity_id=\"Products\",\n",
    "    base_entity_id=\"Purchases\",\n",
    "    index=\"StockCode\",\n",
    "    additional_variables=[\"Description\"]\n",
    ")\n",
    "es[\"Products\"].df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the Stock code is reference as an ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es['Purchases']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create new tables for the customers and orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.normalize_entity(\n",
    "    new_entity_id=\"Customers\",\n",
    "    base_entity_id=\"Purchases\",\n",
    "    index=\"CustomerID\"\n",
    ")\n",
    "es.normalize_entity(\n",
    "    new_entity_id=\"Invoices\",\n",
    "    base_entity_id=\"Purchases\",\n",
    "    index=\"InvoiceNo\",\n",
    "    additional_variables=[\"Country\", \"Cancelled\"]\n",
    ")\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Feature Synthesis\n",
    "\n",
    "Now that we have our entity set, we can use [deep feature synthesis](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf) to generate many features. We can make features for any entity, but sine we're trying to classify customer spending, we'll make features for each customer for each month.\n",
    "\n",
    "To ensure the features are valid for the customer and the month, we'll pass in the labels dataframe that has the cutoff time for each customer for each month. Featuretools will make one row for each customer for each month, with the features for each month derived only from data prior to the cutoff time.\n",
    "\n",
    "When we have an entire dataset and access to all the data, we must prevent ourselves from using all of it when building features since when our model is deployed, it won't have access to data from the future!\n",
    "\n",
    "The requirements are that the table contains the IDs corresponding to the index of the target entity and the table must have cutoff times. Featuretools takes care of the rest, for each month making features using only valid data. The code below generates features for each customer, resulting in a feature matrix where each row consists of one customer for one month, the customers features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, feature_names = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity=\"Customers\",\n",
    "    cutoff_time=labels,\n",
    "    verbose=2,\n",
    "    cutoff_time_in_index=True,\n",
    "    chunk_size=len(labels),\n",
    "    n_jobs=-1,\n",
    "    max_depth=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Look into this more!</font> Why are some `NaN`? For now, let's just fill em. Shouldn't they be zero? Come back to this later if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.fillna(0, inplace=True)\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we described before, we can't use the total columns (or the label for that matter) because these \"look into the future\". We'll do this a little later using `featuretools`. We can also remove the mode of the order ID and product ID. These shouldn't be used for creating features since they are index variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix.drop(columns = [\n",
    "    'MODE(Purchases.InvoiceNo)',\n",
    "    'MODE(Purchases.StockCode)'\n",
    "])\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what featuretools created. We'll use an example customer again.\n",
    "\n",
    "We can see how the numbers change as progress through the year. We might expect that our predictions would get more accurate with time because we're incorporating more information. However, it's also possible that customer behavior changes over time and therefore using all the previous data might not actually be indicative of the customers future actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_id = 12347\n",
    "customer_id = 12352\n",
    "feature_matrix.loc[customer_id, :].sample(10, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add a column for the month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations\n",
    "\n",
    "To approximate what features are useful, we can see if there are any significant correlations between the features and the total amount purchase.\n",
    "\n",
    "First, we have to one-hot encode the categorical features using `get_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method by default doesn't drop a column (creates one-hot with $n$ columns rather than $n-1$).\n",
    "\n",
    "https://github.com/pandas-dev/pandas/issues/12042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = pd.get_dummies(feature_matrix).reset_index()\n",
    "# feature_matrix = pd.get_dummies(feature_matrix, drop_first=True).reset_index()\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = feature_matrix.corr().sort_values(\"Total\")\n",
    "print(\"Strongly negatively correlated features to the total amount spent:\\n\\n{}\".format(correlations[\"Total\"].head()))\n",
    "print(\"\\n\")\n",
    "print(\"Strongly positively correlated features to the total amount spent:\\n\\n{}\".format(correlations[\"Total\"].dropna().tail()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sns.kdeplot` fits and plots a [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE). KDE is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\n",
    "\n",
    "Let $x_{1}, x_{2}, \\ldots, x_{n}$ be a univariate independent and indentically distributed sample drawn from some distribution with an unknown density $f$. The kernel density estimator is\n",
    "\n",
    "<center>$$\\hat{f}_{h}(x) = \\frac{1}{n}\\sum_{i=1}^{n} K_{h} (x - x_{i}) = \\frac{1}{nh}\\sum_{i=1}^{n} K \\frac{x - x_{i}}{h}$$</center>\n",
    "\n",
    "where $K$ is the kernel, a non-negative function, and $h > 0$ is a smoothing parameter called the bandwidth. A kernel with the subscript $h$ is called the scaled kernel and is defined as $Kh(x) = \\frac{1}{h} K(\\frac{x}{h})$.\n",
    "\n",
    "Phew. Don't worry, I had to look that up too. [What it really does is let you create a smooth curve given a set of data](https://mathisonian.github.io/kde/). It's useful if you want to visualize just the \"shape\" of some data, as a kind of continuous replacement for the discrete histogram.\n",
    "\n",
    "The sum of purchase totals prior to the prediction month is higher for those customers who went on to spend $500 in the next month. This is reasonable and explainable feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_display = 'SUM(Purchases.Total)'\n",
    "# column_to_display = 'SUM(Purchases.Quantity)'\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    feature_matrix[(feature_matrix['SUM(Purchases.Total)'] > 0) & (feature_matrix['SUM(Purchases.Total)'] < 1000)],\n",
    "    hue='Label',\n",
    "    size=4,\n",
    "    aspect=3\n",
    ")\n",
    "g.map(sns.kdeplot, column_to_display)\n",
    "g.add_legend()\n",
    "plt.title(\"Distribution of {} by Label\".format(column_to_display))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this for all the columns too. <font color='red'>Okay, so I this isn't working. Deal with it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = feature_matrix.columns\n",
    "# num_columns = len(columns)\n",
    "# print(\"# feature columns: {}\".format(num_columns))\n",
    "# sqrt_columns = np.sqrt(num_columns)\n",
    "# grid_for_plots_x = int(np.ceil(sqrt_columns))\n",
    "# grid_for_plots_y = int(np.floor(sqrt_columns))\n",
    "# print(\"Grid size for plot: {} x {}\".format(grid_for_plots_x, grid_for_plots_y))\n",
    "\n",
    "# plt.figure(figsize=(18, 6))\n",
    "# f, axes = plt.subplots(grid_for_plots_x, grid_for_plots_y)\n",
    "# flattened_axes = [x for y in axes for x in y]\n",
    "# plt.subplots(grid_for_plots_x, grid_for_plots_y)\n",
    "\n",
    "# for column, axis in zip(columns, flattened_axes):\n",
    "#     g = sns.FacetGrid(\n",
    "#         feature_matrix[(feature_matrix['SUM(Purchases.Total)'] > 0) & (feature_matrix['SUM(Purchases.Total)'] < 1000)],\n",
    "#         hue='Label',\n",
    "#         size=4,\n",
    "#         aspect=3\n",
    "#     )\n",
    "#     g.map(sns.kdeplot, column)\n",
    "\n",
    "# g = sns.FacetGrid(\n",
    "#     feature_matrix[(feature_matrix['SUM(Purchases.Total)'] > 0) & (feature_matrix['SUM(Purchases.Total)'] < 1000)],\n",
    "#     hue='Label',\n",
    "#     size=4,\n",
    "#     aspect=3\n",
    "# )\n",
    "# g.map(sns.kdeplot, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the month column for our next plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix['Month'] = feature_matrix['time'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A violin plot is similar to a box plot with a rotated KDE plot on each side. Can you see the differences beween the labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "sns.violinplot(\n",
    "    ax=ax,\n",
    "    x='Month',\n",
    "    y='NUM_UNIQUE(Purchases.InvoiceNo)',\n",
    "    hue='Label',\n",
    "    data=feature_matrix[(feature_matrix['SUM(Purchases.Total)'] > 0) & (feature_matrix['SUM(Purchases.Total)'] < 1000)],\n",
    ")\n",
    "plt.title('Number of Unique Purchases by Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Modelling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Scikit-Learn's [random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    random_state=50,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions for the month of June.\n",
    "\n",
    "The data prep tasks are done using a pipeline for many Python ML packages. Think of this as a thing that takes your input data and formats it such that whatever model you choose can consume the information. Usually, you can setup one pipeline and then use that for any number of different models, which is incredibly convenient.\n",
    "\n",
    "Keep in mind that our training data is dependent upon the time! In other words, we can't use data from the future. We will also see that our model varies from month to month based on the available training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 6\n",
    "test_labels = feature_matrix.loc[feature_matrix['Month'] == month, 'Label']\n",
    "train_labels = feature_matrix.loc[feature_matrix['Month'] < month, 'Label']\n",
    "\n",
    "x_train = feature_matrix[feature_matrix['time'].dt.month < month].drop(columns = [\n",
    "    \"CustomerID\",\n",
    "    \"time\",\n",
    "    \"Month\",\n",
    "    \"Label\",\n",
    "    \"Total\"\n",
    "])\n",
    "x_test = feature_matrix[feature_matrix['time'].dt.month == month].drop(columns = [\n",
    "    \"CustomerID\",\n",
    "    \"time\",\n",
    "    \"Month\",\n",
    "    \"Label\",\n",
    "    \"Total\"\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "transformed_x_train = pipeline.fit_transform(x_train)\n",
    "transformed_x_test = pipeline.fit_transform(x_test)\n",
    "\n",
    "y_train = np.array(train_labels).reshape((-1, ))\n",
    "y_test = np.array(test_labels).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training observations: {}\".format(len(transformed_x_train)))\n",
    "random_forest_model.fit(transformed_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = random_forest_model.predict(transformed_x_test)\n",
    "probs = random_forest_model.predict_proba(transformed_x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute some metrics about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_precision, metrics_recall, metrics_f1, metrics_roc_auc = get_performance_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the cumulative importance of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(x_train.columns)\n",
    "june_feature_importances_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': random_forest_model.feature_importances_\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_feature_importances_df = plot_feature_importances(june_feature_importances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Baseline\n",
    "\n",
    "---\n",
    "\n",
    "Awesome! We have a model. Now what?\n",
    "\n",
    "What if our predictions are no better than luck?\n",
    "\n",
    "Let's make an informed baseline. Let's use the amount the customer spent in the past month to predict how much they'll spend in the next month. Literally, just assume that if they spent more than $500 in the past month, then they'll do that again for this month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_labels = labels.copy()\n",
    "baseline_labels['Month'] = baseline_labels['CutoffTime'].dt.month\n",
    "july_baseline_labels = baseline_labels[baseline_labels['Month'] == 7]\n",
    "june_baseline_labels = baseline_labels[baseline_labels['Month'] == 6]\n",
    "\n",
    "july_baseline_labels = july_baseline_labels.rename(columns = {'Total': 'JulyTotal'})\n",
    "june_baseline_labels = june_baseline_labels.rename(columns = {'Total': 'JuneTotal'})\n",
    "\n",
    "july_baseline_labels = july_baseline_labels.merge(june_baseline_labels[['CustomerID', 'JuneTotal']], on = 'CustomerID', how = 'left')\n",
    "july_baseline_labels['JuneTotal'] = july_baseline_labels['JuneTotal'].fillna(0)\n",
    "july_baseline_labels['PredictedLabel'] = (july_baseline_labels['JuneTotal'] > 500).astype(int)\n",
    "\n",
    "july_baseline_labels['Probability'] = july_baseline_labels['JuneTotal'] / 500\n",
    "    \n",
    "# Set probabilities greater than 1 equal to 1.\n",
    "july_baseline_labels.loc[july_baseline_labels['Probability'] > 1, 'Probability'] = 1\n",
    "    \n",
    "july_baseline_labels.sample(25, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the correlation between the previous months total and the current months?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between totals in July and June: {}\".format(\n",
    "    july_baseline_labels[\"JulyTotal\"].corr(july_baseline_labels[\"JuneTotal\"])))                      \n",
    "sns.lmplot(\"JuneTotal\", \"JulyTotal\", data=july_baseline_labels, fit_reg=False)\n",
    "plt.title(\"July vs. June Spending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_precision, metrics_recall, metrics_f1, metrics_roc_auc = get_performance_metrics(july_baseline_labels['Label'], july_baseline_labels['PredictedLabel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we stack up? Note that I'm going to use the [function](#Functions) I wrote to train the model. It just takes up a lot less space. It's doing the same thing as what you saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    random_state=50,\n",
    "    n_jobs=-1\n",
    ")   \n",
    "random_forest_model, transformed_x_train, transformed_x_test, y_train, y_test = get_trained_model(\n",
    "    model=random_forest_model, \n",
    "    time_period_to_predict=7, # For July.\n",
    "    feature_matrix=feature_matrix\n",
    ")\n",
    "random_forest_model_predictions = random_forest_model.predict(transformed_x_test)\n",
    "random_forest_model_probabilities = random_forest_model.predict_proba(transformed_x_test)[:, 1]\n",
    "\n",
    "print(\"\\n\") # Look pretty!\n",
    "_ = get_performance_metrics(\n",
    "    truth=y_test,\n",
    "    predictions=random_forest_model_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>How can I adjust threshold to tune recall, precision and F1 score?</font>\n",
    "\n",
    "Plotting the ROC gives a great idea for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate false positive rates and true positive rates\n",
    "base_fpr, base_tpr, _ = roc_curve(july_baseline_labels['Label'], july_baseline_labels['Probability'])\n",
    "model_fpr, model_tpr, _ = roc_curve(feature_matrix[feature_matrix['Month'] == 7]['Label'], random_forest_model_probabilities)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "plt.legend();\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We're slightly better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write up a function to do this all quicker. Here's a comparison for June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_baseline_to_auto(6, feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_baseline_to_auto(12, feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "April (anything before this dies due to lack of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_baseline_to_auto(4, feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Deep Feature Synthesis\n",
    "\n",
    "---\n",
    "\n",
    "Not bad for being automated. Can we do better?\n",
    "\n",
    "Let's try adding in a few more primitives and more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = labels.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_matrix, feature_names = ft.dfs(\n",
    "#     entityset=es,\n",
    "#     target_entity='Customers',\n",
    "#     agg_primitives=[\n",
    "#         'std',\n",
    "#         'max',\n",
    "#         'min',\n",
    "#         'mode',\n",
    "#         'mean',\n",
    "#         'skew',\n",
    "#         'last',\n",
    "#         'avg_time_between'\n",
    "#     ],\n",
    "#     trans_primitives=[\n",
    "#         'cum_mean',\n",
    "#         'cum_sum',\n",
    "#         'day',\n",
    "#         'month',\n",
    "#         'hour',\n",
    "#         'weekend'\n",
    "#     ],\n",
    "#     n_jobs=-1,\n",
    "#     chunk_size=100,\n",
    "#     max_depth=2,\n",
    "#     cutoff_time=labels,\n",
    "#     cutoff_time_in_index=True,\n",
    "#     verbose=1\n",
    "# )\n",
    "# feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "1. Dua, D.; Karra Taniskidou, E. (2017). UCI Machine Learning Repository http://archive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "1. Featuretools (2019), GitHub repository, https://github.com/Featuretools.\n",
    "1. Katner, J. M.; Veeramachaneni, K. Deep feature synthesis: Towards automating data science endeavors, *2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)*, https://doi.org/10.1109/DSAA.2015.7344858.\n",
    "1. Conlen, M. Kernel Density Estimation, https://mathisonian.github.io/kde."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
